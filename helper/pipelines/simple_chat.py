from datetime import datetime
from openai import OpenAI
from config import Config
from logger import logger
from helper.openai import openai_chat_completion_with_retry
from llama_index.core.query_pipeline import QueryPipeline, InputComponent
from llama_index.storage.chat_store.redis import RedisChatStore
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.llms.openai import OpenAI
from llama_index.core.llms import ChatMessage
from helper.pipelines import ResponseWithChatHistory


def simple_chat_pipeline(
    customer_query: str, chat_uuid: str, model: str = Config.DEFAULT_OPENAI_MODEL
) -> str:
    """
    Query the csv file using the query pipeline.

    Args:
        customer_query (str): The query requested by the customer.
        model (str, optional): The OpenAI model to use for generating the response. Defaults to Config.DEFAULT_OPENAI_MODEL.

    Returns:
        str: The response generated by the OpenAI model.
    """
    chat_store = RedisChatStore(redis_url=Config.REDIS_STORE_URL, ttl=300)
    chat_memory = ChatMemoryBuffer.from_defaults(
        chat_store=chat_store, chat_store_key=chat_uuid, token_limit=5000
    )

    llm = OpenAI(model=model, temperature=0.0, top_p=0.2, api_key=Config.OPENAI_API_KEY)

    response_component = ResponseWithChatHistory(
        llm=llm,
        context_prompt="""
            The customer has requested the following query:
            {query_str}
            response:
            """,
        system_prompt="""
            You are an assistance with data analyst and database expertise. You should
            answer the customer query and be helpful.
            """,
    )
    input_component = InputComponent()
    chat_history = chat_memory.get()
    logger.debug(f"Chat history: {chat_history}")

    p = QueryPipeline(verbose=True)
    p.add_modules(
        {
            "input": input_component,
            "response_component": response_component,
        }
    )
    p.add_link(
        "input",
        "response_component",
        src_key="query_str",
        dest_key="query_str",
    )
    p.add_link(
        "input", "response_component", src_key="chat_history", dest_key="chat_history"
    )

    response = p.run(query_str=customer_query, chat_history=chat_history)

    # update the memory
    user_msg = ChatMessage(role="user", content=customer_query)
    chat_memory.put(user_msg)
    chat_memory.put(response.message)

    # response_text = openai_chat_completion_with_retry(
    #     system_prompt=system_prompt,
    #     user_prompt=query_prompt,
    #     model=model,
    # )

    return response.message.content
